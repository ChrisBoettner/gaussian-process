{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Float64 for more stable matrix inversions.\n",
    "from jax import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "\n",
    "from jax import jit, tree_map\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jaxtyping import install_import_hook, Array\n",
    "from copy import deepcopy\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from gpjax.base import meta_leaves\n",
    "from jax.flatten_util import ravel_pytree\n",
    "from jax.stages import Wrapped\n",
    "import warnings\n",
    "import optax as ox\n",
    "from tqdm import tqdm\n",
    "\n",
    "with install_import_hook(\"gpjax\", \"beartype.beartype\"):\n",
    "    import gpjax as gpx\n",
    "    from gpjax.kernels import Constant, Linear, RBF, Periodic, PoweredExponential\n",
    "\n",
    "key = jr.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from beartype.typing import Union\n",
    "import jax.numpy as jnp\n",
    "from jaxtyping import Float\n",
    "import tensorflow_probability.substrates.jax.bijectors as tfb\n",
    "import tensorflow_probability.substrates.jax.distributions as tfd\n",
    "\n",
    "from gpjax.base import param_field\n",
    "from gpjax.kernels.base import AbstractKernel\n",
    "from gpjax.kernels.stationary.utils import squared_distance\n",
    "from gpjax.typing import (\n",
    "    Array,\n",
    "    ScalarFloat,\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OrnsteinUhlenbeck(AbstractKernel):\n",
    "    r\"\"\"The Ornstein-Uhlenbeck kernel.\"\"\"\n",
    "\n",
    "    lengthscale: Union[ScalarFloat, Float[Array, \" D\"]] = param_field(\n",
    "        jnp.array(1.0), bijector=tfb.Softplus()\n",
    "    )\n",
    "    variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())\n",
    "    name: str = \"OU\"\n",
    "\n",
    "    def __call__(self, x: Float[Array, \" D\"], y: Float[Array, \" D\"]) -> ScalarFloat:\n",
    "        r\"\"\"Compute the OU kernel between a pair of arrays.\n",
    "\n",
    "        Evaluate the kernel on a pair of inputs $`(x, y)`$ with lengthscale parameter\n",
    "        $`\\ell`$ and variance $`\\sigma^2`$:\n",
    "        ```math\n",
    "        k(x,y)=\\sigma^2\\exp\\Bigg(- \\frac{\\lVert x - y \\rVert_2}{2 \\ell^2} \\Bigg)\n",
    "        ```\n",
    "\n",
    "        Args:\n",
    "            x (Float[Array, \" D\"]): The left hand argument of the kernel function's call.\n",
    "            y (Float[Array, \" D\"]): The right hand argument of the kernel function's call.\n",
    "\n",
    "        Returns:\n",
    "            ScalarFloat: The value of $`k(x, y)`$.\n",
    "        \"\"\"\n",
    "        x = self.slice_input(x) / self.lengthscale\n",
    "        y = self.slice_input(y) / self.lengthscale\n",
    "        K = self.variance * jnp.exp(-0.5 * jnp.sum(jnp.abs(x - y)))\n",
    "        return K.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(\n",
    "        self,\n",
    "        posterior: gpx.gps.AbstractPosterior,\n",
    "        max_log_likelihood: Optional[float] = None,\n",
    "        n_data: Optional[int] = None,\n",
    "        parent=Optional[\"Node\"],\n",
    "    ):\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "\n",
    "        self.update(\n",
    "            posterior,\n",
    "            max_log_likelihood,\n",
    "            n_data,\n",
    "        )\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        posterior: gpx.gps.AbstractPosterior,\n",
    "        max_log_likelihood: Optional[float] = None,\n",
    "        n_data: Optional[int] = None,\n",
    "    ):\n",
    "        self.posterior = posterior\n",
    "\n",
    "        self.n_parameter = sum(\n",
    "            leaf[0][\"trainable\"]\n",
    "            for leaf in meta_leaves(posterior)\n",
    "            if isinstance(leaf[0], dict)\n",
    "        )  # number of trainable parameter\n",
    "\n",
    "        if n_data is not None:\n",
    "            self.n_data = np.log(n_data)\n",
    "\n",
    "        if max_log_likelihood is not None:\n",
    "            self.max_log_likelihood = max_log_likelihood\n",
    "            if self.n_data is not None:\n",
    "                self.bic = self.n_parameter * self.n_data - 2 * self.max_log_likelihood\n",
    "\n",
    "    def add_child(\n",
    "        self,\n",
    "        node: \"Node\",\n",
    "    ):\n",
    "        self.children.append(node)\n",
    "\n",
    "\n",
    "class KernelSearch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_library: list[gpx.kernels.AbstractKernel],\n",
    "        data: gpx.Dataset,\n",
    "        obs_stddev: float | Array = 1,\n",
    "        fit_obs_stddev: bool = False,\n",
    "        likelihood: Optional[gpx.likelihoods.AbstractLikelihood] = None,\n",
    "        objective: Optional[gpx.objectives.AbstractObjective | Wrapped] = None,\n",
    "        mean_function: Optional[gpx.mean_functions.AbstractMeanFunction] = None,\n",
    "        root_kernel: Optional[gpx.kernels.AbstractKernel] = None,\n",
    "        fitting_mode: str = \"scipy\",\n",
    "        num_iters: int = 1000,\n",
    "        parallelise: bool = True,\n",
    "        n_cores: int = 8,\n",
    "        verbosity: int = 1,\n",
    "    ):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        kernel_library : list[gpx.kernels.AbstractKernel]\n",
    "            _description_\n",
    "        data : gpx.Dataset\n",
    "            _description_\n",
    "        obs_stddev : float | Array, optional\n",
    "            _description_, is ignored if custom likelihood is given, by default 1\n",
    "        fit_obs_stddev : bool, optional\n",
    "            _description_, is ignored if custom likelihood is given, by default False\n",
    "        likelihood : Optional[gpx.likelihoods.AbstractLikelihood], optional\n",
    "            _description_, by default None, which defaults to the Gaussian likelihood with standard deviation given by obs_stddev.\n",
    "        objective : Optional[gpx.objectives.AbstractObjective  |  Wrapped], optional\n",
    "            _description_, by default None, which defaults to a jit-compiled negative marginal log-likelihood (for a gaussian likelihood), must be adapted if the likelihood is not Gaussian\n",
    "        mean_function : Optional[gpx.mean_functions.AbstractMeanFunction], optional\n",
    "            _description_, by default None, which sets the mean to zero\n",
    "        root_kernel : gpx.kernels.AbstractKernel\n",
    "        _description_\n",
    "        fitting_mode : str, optional\n",
    "            _description_, by default \"scipy\"\n",
    "        num_iters : int, optional\n",
    "            _description_, by default 1000\n",
    "        parallelise : bool, optional\n",
    "            _description_, by default True\n",
    "        n_cores : int, optional\n",
    "            _description_, by default 8\n",
    "        verbosity : int, optional\n",
    "            _description_, by default 1\n",
    "        \"\"\"\n",
    "        if isinstance(obs_stddev, float):\n",
    "            obs_stddev = jnp.array(obs_stddev)\n",
    "        if likelihood is None:\n",
    "            likelihood = gpx.likelihoods.Gaussian(\n",
    "                num_datapoints=data.n, obs_stddev=obs_stddev\n",
    "            )\n",
    "            likelihood = likelihood.replace_trainable(obs_stddev=fit_obs_stddev)  # type: ignore\n",
    "        if objective is None:\n",
    "            objective = jit(gpx.objectives.ConjugateMLL(negative=True))\n",
    "        if mean_function is None:\n",
    "            mean_function = gpx.mean_functions.Zero()\n",
    "\n",
    "        self.likelihood = likelihood\n",
    "        self.objective = objective\n",
    "        self.data = data\n",
    "        self.kernel_library = kernel_library\n",
    "\n",
    "        self.fitting_mode = fitting_mode\n",
    "        self.num_iters = num_iters\n",
    "\n",
    "        self.parallelise = parallelise\n",
    "        self.n_cores = n_cores\n",
    "\n",
    "        self.verbosity = verbosity\n",
    "\n",
    "        self.root = [\n",
    "            Node(\n",
    "                likelihood\n",
    "                * gpx.gps.Prior(\n",
    "                    mean_function=mean_function,\n",
    "                    kernel=self._const_kernel() * kernel,\n",
    "                )\n",
    "            )\n",
    "            for kernel in (kernel_library if root_kernel is None else [root_kernel])\n",
    "        ]\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _const_kernel(trainable=False):\n",
    "        return Constant(constant=jnp.array(1.0)).replace_trainable(constant=trainable)  # type: ignore\n",
    "\n",
    "    def fit(self, posterior) -> tuple[gpx.gps.AbstractPosterior, float]:\n",
    "        if self.fitting_mode == \"scipy\":\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                optimized_posterior, history = gpx.fit_scipy(\n",
    "                    model=posterior,\n",
    "                    objective=self.objective,\n",
    "                    train_data=self.data,\n",
    "                    max_iters=self.num_iters,\n",
    "                    verbose=self.verbosity >= 2\n",
    "                )\n",
    "        elif self.fitting_mode == \"adam\":\n",
    "            static_tree = tree_map(lambda x: not x, posterior.trainables)\n",
    "            optim = ox.chain(\n",
    "                ox.adamw(learning_rate=3e-4),\n",
    "                ox.masked(\n",
    "                    ox.set_to_zero(),\n",
    "                    static_tree,\n",
    "                ),\n",
    "            )\n",
    "            optimized_posterior, history = gpx.fit(\n",
    "                model=posterior,\n",
    "                objective=self.objective,\n",
    "                train_data=self.data,\n",
    "                optim=optim,\n",
    "                key=key,\n",
    "                num_iters=self.num_iters,\n",
    "                verbose=self.verbosity >= 2\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"'fitting_mode' must be 'scipy' or 'adam'.\")\n",
    "\n",
    "        max_log_likelihood = -float(history[-1])\n",
    "        return optimized_posterior, max_log_likelihood\n",
    "\n",
    "\n",
    "    # def fit(self, posterior) -> tuple[gpx.gps.AbstractPosterior, float]:\n",
    "    #     static_tree = tree_map(lambda x: not (x), posterior.trainables)\n",
    "    #     optim = ox.chain(\n",
    "    #         ox.adamw(learning_rate=3e-4),\n",
    "    #         ox.masked(\n",
    "    #             ox.set_to_zero(),\n",
    "    #             static_tree,\n",
    "    #         ),\n",
    "    #     )\n",
    "    #     try:\n",
    "    #         optimized_posterior, history = gpx.fit(\n",
    "    #             model=posterior,\n",
    "    #             objective=self.objective,\n",
    "    #             train_data=self.data,\n",
    "    #             optim=optim,\n",
    "    #             key=key,\n",
    "    #             num_iters=self.num_iters,\n",
    "    #             verbose=True if self.verbosity >= 2 else False,\n",
    "    #         )\n",
    "    #         max_log_likelihood = -float(history[-1])\n",
    "    #     except FloatingPointError:\n",
    "    #         return posterior, -np.inf\n",
    "    #     return optimized_posterior, max_log_likelihood\n",
    "\n",
    "    def expand_node(self, node):\n",
    "        for kernel_operation in [gpx.kernels.ProductKernel, gpx.kernels.SumKernel]:\n",
    "            for ker in self.kernel_library:\n",
    "                kernel = deepcopy(node.posterior.prior.kernel)\n",
    "\n",
    "                new_kernel = deepcopy(ker)  # type: ignore\n",
    "                if kernel_operation == gpx.kernels.SumKernel:\n",
    "                    # create new additive term with tracer constant\n",
    "                    # the first kernel in the new term has a trainable constant\n",
    "                    new_kernel = gpx.kernels.ProductKernel(kernels=[self._const_kernel(), new_kernel])  # type: ignore\n",
    "                if kernel_operation == gpx.kernels.ProductKernel:\n",
    "                    # further kernels have variance fixed, so that we only have one constant\n",
    "                    try:\n",
    "                        new_kernel = new_kernel.replace_trainable(variance=False)  # type: ignore\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "                composite_kernel = kernel_operation(kernels=[kernel, new_kernel])  # type: ignore\n",
    "\n",
    "                new_prior = gpx.gps.Prior(\n",
    "                    mean_function=node.posterior.prior.mean_function,\n",
    "                    kernel=composite_kernel,\n",
    "                )\n",
    "                new_posterior = self.likelihood * new_prior\n",
    "                node.add_child(Node(new_posterior, parent=node))\n",
    "\n",
    "    def compute_layer(self, layer, current_depth):\n",
    "        if self.verbosity == 1:\n",
    "            for node in tqdm(layer, desc=f\"Fitting Layer {current_depth +1}\"):\n",
    "                node.update(*self.fit(node.posterior), self.data.n)\n",
    "        else:\n",
    "            [node.update(*self.fit(node.posterior), self.data.n) for node in layer]\n",
    "\n",
    "    def select_top_nodes(self, layer, bic_threshold, n_leafs):\n",
    "        sorted_tuple = sorted((node.bic, node) for node in layer)\n",
    "        # return first n_leafs nodes\n",
    "        top_nodes = [node for _, node in sorted_tuple][:n_leafs]\n",
    "        # filter for bic threshold\n",
    "        top_nodes = [node for node in top_nodes if node.bic < bic_threshold]\n",
    "        if top_nodes:\n",
    "            self.best_model = top_nodes[0]\n",
    "        return top_nodes\n",
    "\n",
    "    def expand_layer(self, layer):\n",
    "        next_layer = []\n",
    "        for node in layer:\n",
    "            self.expand_node(node)\n",
    "            next_layer.extend(node.children)\n",
    "        return next_layer\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        depth: int = 10,\n",
    "        n_leafs: int = 3,\n",
    "    ):\n",
    "        layer = self.root\n",
    "\n",
    "        current_depth = 0\n",
    "        bic_threshold = np.inf\n",
    "        for current_depth in range(depth):\n",
    "            self.compute_layer(layer, current_depth)\n",
    "            if current_depth == 0:\n",
    "                best_model = sorted((node.bic, node) for node in layer)[0][1]\n",
    "\n",
    "            current_bics = sorted([node.bic for node in layer])\n",
    "            if current_bics[0] > bic_threshold:\n",
    "                if self.verbosity >= 1:\n",
    "                    print(\"No more improvements found! Terminating early..\\n\")\n",
    "                    break\n",
    "\n",
    "            if self.verbosity >= 1:\n",
    "                print(f\"Layer {current_depth+1} || Current BICs: {current_bics}\")\n",
    "\n",
    "            layer = self.select_top_nodes(layer, bic_threshold, n_leafs)\n",
    "            bic_threshold = current_bics[0]  # min bic of current layer\n",
    "            best_model = layer[0]\n",
    "            layer = self.expand_layer(layer)\n",
    "\n",
    "        if self.verbosity >= 1:\n",
    "            print(f\"Terminated on layer: {current_depth+1}.\")\n",
    "            print(f\"Final log likelihood: {best_model.max_log_likelihood}\")\n",
    "            print(f\"Final number of model paramter: {best_model.n_parameter}\")\n",
    "        return best_model.posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "n = 100\n",
    "noise = 0.3\n",
    "\n",
    "key, subkey = jr.split(key)\n",
    "x = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1)\n",
    "f = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\n",
    "signal = f(x)\n",
    "y = signal + jr.normal(subkey, shape=signal.shape) * noise\n",
    "\n",
    "D = gpx.Dataset(X=x, y=y)\n",
    "\n",
    "xtest = jnp.linspace(-3.5, 3.5, 500).reshape(-1, 1)\n",
    "ytest = f(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_library = [\n",
    "    Linear(),\n",
    "    RBF(),\n",
    "    OrnsteinUhlenbeck(),\n",
    "    Periodic(),\n",
    "    PoweredExponential(power=jnp.array(0.8)),\n",
    "]  # default powered exponential has infinite parameter for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todos :\n",
    "# add scipy fit\n",
    "# implement Ornstein-Uhlenbeck kernel\n",
    "# parallelize\n",
    "# test\n",
    "\n",
    "# rescale input! (i.e. if mean func is zero, should be centered at zero, maybe rather do that in data fit rountine though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Layer 1:   0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TracerBoolConversionError",
     "evalue": "Attempted boolean conversion of traced array with shape bool[]..\nThe error occurred while tracing the function step at /home/chris/mambaforge/envs/hobby/lib/python3.10/site-packages/gpjax/fit.py:152 for scan. This value became a tracer due to JAX operations on these lines:\n\n  operation a\u001b[35m:bool[]\u001b[39m = eq b c\n    from line <string>:4 (__eq__)\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTracerBoolConversionError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m tree \u001b[38;5;241m=\u001b[39m KernelSearch(\n\u001b[1;32m      2\u001b[0m     kernel_library,\n\u001b[1;32m      3\u001b[0m     data\u001b[38;5;241m=\u001b[39mD,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     num_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_leafs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 256\u001b[0m, in \u001b[0;36mKernelSearch.search\u001b[0;34m(self, depth, n_leafs)\u001b[0m\n\u001b[1;32m    254\u001b[0m bic_threshold \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m current_depth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth):\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_depth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m current_depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    258\u001b[0m         best_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m((node\u001b[38;5;241m.\u001b[39mbic, node) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m layer)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[3], line 225\u001b[0m, in \u001b[0;36mKernelSearch.compute_layer\u001b[0;34m(self, layer, current_depth)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m tqdm(layer, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting Layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_depth\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 225\u001b[0m         node\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposterior\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mn)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     [node\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(node\u001b[38;5;241m.\u001b[39mposterior), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mn) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m layer]\n",
      "Cell \u001b[0;32mIn[3], line 156\u001b[0m, in \u001b[0;36mKernelSearch.fit\u001b[0;34m(self, posterior)\u001b[0m\n\u001b[1;32m    148\u001b[0m     static_tree \u001b[38;5;241m=\u001b[39m tree_map(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;129;01mnot\u001b[39;00m x, posterior\u001b[38;5;241m.\u001b[39mtrainables)\n\u001b[1;32m    149\u001b[0m     optim \u001b[38;5;241m=\u001b[39m ox\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    150\u001b[0m         ox\u001b[38;5;241m.\u001b[39madamw(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m),\n\u001b[1;32m    151\u001b[0m         ox\u001b[38;5;241m.\u001b[39mmasked(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m         ),\n\u001b[1;32m    155\u001b[0m     )\n\u001b[0;32m--> 156\u001b[0m     optimized_posterior, history \u001b[38;5;241m=\u001b[39m \u001b[43mgpx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposterior\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjective\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_iters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbosity\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfitting_mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscipy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/hobby/lib/python3.10/site-packages/gpjax/fit.py:171\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, objective, train_data, optim, key, num_iters, batch_size, log_rate, verbose, unroll, safe)\u001b[0m\n\u001b[1;32m    168\u001b[0m scan \u001b[38;5;241m=\u001b[39m vscan \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;28;01melse\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mscan\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Optimisation loop.\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m (model, _), history \u001b[38;5;241m=\u001b[39m \u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43miter_keys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munroll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munroll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Constrained space.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconstrain()\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/hobby/lib/python3.10/site-packages/gpjax/fit.py:160\u001b[0m, in \u001b[0;36mfit.<locals>.step\u001b[0;34m(carry, key)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     batch \u001b[38;5;241m=\u001b[39m train_data\n\u001b[0;32m--> 160\u001b[0m loss_val, loss_gradient \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m updates, opt_state \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mupdate(loss_gradient, opt_state, model)\n\u001b[1;32m    162\u001b[0m model \u001b[38;5;241m=\u001b[39m ox\u001b[38;5;241m.\u001b[39mapply_updates(model, updates)\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/hobby/lib/python3.10/site-packages/gpjax/fit.py:140\u001b[0m, in \u001b[0;36mfit.<locals>.loss\u001b[0;34m(model, batch)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(model: Module, batch: Dataset) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ScalarFloat:\n\u001b[1;32m    139\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstop_gradient()\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/hobby/lib/python3.10/weakref.py:509\u001b[0m, in \u001b[0;36mWeakKeyDictionary.setdefault\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remove\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:4\u001b[0m, in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/hobby/lib/python3.10/site-packages/jax/_src/core.py:1510\u001b[0m, in \u001b[0;36mconcretization_function_error.<locals>.error\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[0;32m-> 1510\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m TracerBoolConversionError(arg)\n",
      "\u001b[0;31mTracerBoolConversionError\u001b[0m: Attempted boolean conversion of traced array with shape bool[]..\nThe error occurred while tracing the function step at /home/chris/mambaforge/envs/hobby/lib/python3.10/site-packages/gpjax/fit.py:152 for scan. This value became a tracer due to JAX operations on these lines:\n\n  operation a\u001b[35m:bool[]\u001b[39m = eq b c\n    from line <string>:4 (__eq__)\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError"
     ]
    }
   ],
   "source": [
    "tree = KernelSearch(\n",
    "    kernel_library,\n",
    "    data=D,\n",
    "    obs_stddev=0.3,\n",
    "    verbosity=1,\n",
    "    fitting_mode=\"adam\",\n",
    "    num_iters=1000,\n",
    ")\n",
    "\n",
    "model = tree.search(depth=5, n_leafs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = model.predict(train_data=D, test_inputs=signal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hobby",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
